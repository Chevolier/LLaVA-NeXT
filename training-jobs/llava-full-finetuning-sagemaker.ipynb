{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA Training Scripts for SageMaker\n",
    "\n",
    "Create a SageMaker training script which is adapted from LLaVA/scripts/v1_5/finetune_task.sh.\n",
    "According to LLaVA, per_device_train_batch_size * gradient_accumulation_steps * number of devices = 128\n",
    "This setting is tested on ml.p4d.24xlarge (8 * A100[40G])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 sync ./data/ s3://YOUR_S3_BUCKET/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile LLaVA/finetune-llava-video.sh\n",
    "\n",
    "#!/bin/bash\n",
    "export WANDB_MODE=offline\n",
    "\n",
    "WORKING_DIR=/opt/ml/code\n",
    "SM_WORKING_DIR=/opt/ml/model\n",
    "\n",
    "#The related information about multi-nodes cluster.\n",
    "MASTER_HOST=$SM_MASTER\n",
    "MASTER_ADDR=$SM_MASTER_ADDR\n",
    "MASTER_PORT=\"23456\"\n",
    "NNODES=\"$NODE_NUMBER\"\n",
    "NODE_RANK=\"$NODE_INDEX\"\n",
    "GPUS_PER_NODE=\"$SM_NUM_GPUS\"\n",
    "\n",
    "echo \"NNODES: ${NNODES}\"\n",
    "echo \"NODE_RANK: ${NODE_RANK}\"\n",
    "echo \"GPUS_PER_NODE: ${GPUS_PER_NODE}\"\n",
    "echo \"job_id: ${job_id}\"\n",
    "\n",
    "LLM_VERSION=\"Qwen/Qwen2-7B-Instruct\"\n",
    "LLM_VERSION_CLEAN=\"Qwen2-7B-Instruct\"\n",
    "VISION_MODEL_VERSION=\"google/siglip-so400m-patch14-384\"\n",
    "VISION_MODEL_VERSION_CLEAN=\"siglip-so400m-patch14-384\"\n",
    "\n",
    "PROMPT_VERSION=plain\n",
    "PRETRAIN_DATA_VERSION=\"blip558k\"\n",
    "\n",
    "BASE_RUN_NAME=\"LLaVA-Video-7B-Qwen2\"\n",
    "echo \"BASE_RUN_NAME: ${BASE_RUN_NAME}\"\n",
    "\n",
    "PROMPT_VERSION=\"qwen_1_5\"\n",
    "MID_RUN_NAME=\"llavanext-${VISION_MODEL_VERSION_CLEAN}-${LLM_VERSION_CLEAN}-ov_to_video_am9_aug17\"\n",
    "PREV_STAGE_CHECKPOINT=\"lmms-lab/LLaVA-Video-7B-Qwen2\"\n",
    "echo \"PREV_STAGE_CHECKPOINT: ${PREV_STAGE_CHECKPOINT}\"\n",
    "echo \"MID_RUN_NAME: ${MID_RUN_NAME}\"\n",
    "\n",
    "export AV_LOG_LEVEL=error  # Suppress FFmpeg info/warning messages\n",
    "export PYTHONWARNINGS=\"ignore::UserWarning\"  # Filter Python warnings\n",
    "\n",
    "# --mm_tunable_parts=\"mm_vision_tower,mm_mlp_adapter,mm_language_model\"\n",
    "\n",
    "ACCELERATE_CPU_AFFINITY=1 torchrun --nproc_per_node=\"${GPUS_PER_NODE}\" --nnodes=\"${NNODES}\" --node_rank=\"${NODE_RANK}\" --master_addr=\"${MASTER_ADDR}\" --master_port=\"${MASTER_PORT}\" \\\n",
    "    llava/train/train_mem.py \\\n",
    "    --deepspeed scripts/zero3.json \\\n",
    "    --model_name_or_path $PREV_STAGE_CHECKPOINT \\\n",
    "    --version $PROMPT_VERSION \\\n",
    "    --data_path /opt/ml/input/data/training/train_formatted.json \\\n",
    "    --image_folder /opt/ml/input/data/training \\\n",
    "    --video_folder /opt/ml/input/data/training \\\n",
    "    --mm_tunable_parts=\"mm_language_model\" \\\n",
    "    --mm_vision_tower_lr=2e-6 \\\n",
    "    --vision_tower ${VISION_MODEL_VERSION} \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --group_by_modality_length True \\\n",
    "    --image_aspect_ratio anyres_max_9 \\\n",
    "    --image_grid_pinpoints  \"(1x1),...,(6x6)\" \\\n",
    "    --mm_patch_merge_type spatial_unpad \\\n",
    "    --bf16 True \\\n",
    "    --run_name $MID_RUN_NAME \\\n",
    "    --output_dir /opt/ml/checkpoints/${job_id} \\\n",
    "    --num_train_epochs 20 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 2 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 25 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate 1e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 22768 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 2 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --torch_compile True \\\n",
    "    --torch_compile_backend \"inductor\" \\\n",
    "    --dataloader_drop_last True \\\n",
    "    --frames_upbound 32 \\\n",
    "    --mm_newline_position grid \\\n",
    "    --add_time_instruction True \\\n",
    "    --force_sample True \\\n",
    "    --mm_spatial_pool_stride 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sagemaker session and get the training data s3 uri\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "import os\n",
    "\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = \"sagemaker-us-west-2-452145973879\"\n",
    "PREFIX = \"datasets/hualai-video/hualai_sft_data/\"\n",
    "s3uri = os.path.join(\"s3://\", BUCKET, PREFIX)\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"sagemaker bucket: {BUCKET}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"data uri: {s3uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique training job id\n",
    "from time import gmtime, strftime\n",
    "job_id = \"llava-video-task-llm-\"+strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = {\n",
    "        'job_id': job_id\n",
    "}\n",
    "\n",
    "# Define metrics definitions, such metrics will be extracted from training script's printed logs and send to cloudwatch\n",
    "metric_definitions=[\n",
    "        {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_runtime', 'Regex': \"'train_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_samples_per_second', 'Regex': \"'train_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_steps_per_second', 'Regex': \"'train_steps_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'train_loss', 'Regex': \"'train_loss': ([0-9]+(.|e\\-)[0-9]+),?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point the training data to the s3 uri. Use FastFile to \"mount\" the s3 files directly instead of copying to local disk\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "training_input = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=s3uri,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "image_uri = \"452145973879.dkr.ecr.us-west-2.amazonaws.com/llava-video\"\n",
    "# image_uri = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/pytorch-training:2.4.0-gpu-py311-cu124-ubuntu22.04-sagemaker\"\n",
    "instance_type = 'ml.g6e.48xlarge' # 'ml.g6e.12xlarge' # 'ml.p4d.24xlarge' \n",
    "use_spot_instances = False\n",
    "max_run = 36000  # seconds, max 432,000 seconds (5 days)\n",
    "max_wait = 40000 if use_spot_instances else None # seconds, max 3,600,000 seconds (1,000 hours)\n",
    "keep_alive_period_in_seconds = None\n",
    "\n",
    "output_uri = os.path.join(\"s3://\", BUCKET, job_id, \"output\")\n",
    "checkpoint_uri = os.path.join(\"s3://\", BUCKET, job_id, \"checkpoints\")\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='start.py',\n",
    "                                    source_dir='./LLaVA',\n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=1,\n",
    "                                    py_version='py310',\n",
    "                                    image_uri=image_uri,\n",
    "                                    role=ROLE,\n",
    "                                    metric_definitions=metric_definitions,\n",
    "                                    environment=environment,\n",
    "                                    use_spot_instances=use_spot_instances,\n",
    "                                    max_run=max_run,\n",
    "                                    max_wait=max_wait,\n",
    "                                    output_path=output_uri,\n",
    "                                    checkpoint_s3_uri=checkpoint_uri,\n",
    "                                    keep_alive_period_in_seconds=keep_alive_period_in_seconds,\n",
    "                                   )\n",
    "\n",
    "huggingface_estimator.fit({'training': training_input}, job_name=job_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
